{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIF3Uj5jocZmaAD+DEe7wV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ghustavo516/LingoLanguage-API/blob/main/Desenvolvendo_o_primeiro_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEJCajV_9Cz5",
        "outputId": "d24c290f-9acb-4c05-e62d-3edc82c10796"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "texto = enc.encode(\"Olá, vamos aprender a desenvolver um LLM\")"
      ],
      "metadata": {
        "id": "bAFq4oLV-XbK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Usando causal mask - GPT**"
      ],
      "metadata": {
        "id": "7Qj3t0gTJDRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texto) -1):\n",
        "  x = texto[:i]\n",
        "  y = texto[i+1]\n",
        "  if x != []:\n",
        "    print(f'Quando os dados forem: {x} o alvo é {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAyjCP3ZGtEW",
        "outputId": "9fc8350f-9361-4419-f520-4e3896e28126"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quando os dados forem: [30098] o alvo é 11\n",
            "Quando os dados forem: [30098, 6557] o alvo é 410\n",
            "Quando os dados forem: [30098, 6557, 11] o alvo é 321\n",
            "Quando os dados forem: [30098, 6557, 11, 410] o alvo é 418\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321] o alvo é 2471\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418] o alvo é 13287\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471] o alvo é 257\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287] o alvo é 748\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257] o alvo é 268\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748] o alvo é 10396\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268] o alvo é 332\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396] o alvo é 23781\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332] o alvo é 27140\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781] o alvo é 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Usando o random mask - BERT**"
      ],
      "metadata": {
        "id": "3mjsTg2VIBP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "for i in range(len(texto)):\n",
        "  x = texto\n",
        "  y = texto.copy()\n",
        "  idx_mask = randint(0, len(texto) -1)\n",
        "  y[idx_mask] = '<mask>'\n",
        "  print(f'Quando os dados forem: {x} o alvo é {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMummFpMHqUw",
        "outputId": "c5090685-c24e-4dbc-9793-a56de412b80d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, '<mask>', 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, '<mask>']\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, '<mask>', 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, '<mask>', 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, '<mask>']\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, '<mask>', 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, '<mask>', 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, '<mask>', 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, '<mask>']\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, '<mask>', 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, '<mask>', 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, '<mask>', 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, '<mask>']\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é ['<mask>', 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, '<mask>', 257, 748, 268, 10396, 332, 23781, 27140, 44]\n",
            "Quando os dados forem: [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, 332, 23781, 27140, 44] o alvo é [30098, 6557, 11, 410, 321, 418, 2471, 13287, 257, 748, 268, 10396, '<mask>', 23781, 27140, 44]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LREFZIbPKbKY",
        "outputId": "752af219-1674-4b48-98b3-9f1c62843181"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zFuP_rI5Gswz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ./sample_data/crepusculoDosIdolos.txt https://raw.githubusercontent.com/mfmarlonferrari/NietzscheLLM/main/crepusculoDosIdolos.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd-qc7OmN5ay",
        "outputId": "f292931f-2569-4ad1-e521-8964acc91e84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-23 20:18:48--  https://raw.githubusercontent.com/mfmarlonferrari/NietzscheLLM/main/crepusculoDosIdolos.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162098 (158K) [text/plain]\n",
            "Saving to: ‘./sample_data/crepusculoDosIdolos.txt’\n",
            "\n",
            "./sample_data/crepu 100%[===================>] 158.30K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-04-23 20:18:48 (6.52 MB/s) - ‘./sample_data/crepusculoDosIdolos.txt’ saved [162098/162098]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = './sample_data/'\n",
        "dados_treino = 'crepusculoDosIdolos.txt'"
      ],
      "metadata": {
        "id": "1b0a3QRtOp9d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files=[PATH+dados_treino], vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\", #começo de linha,\n",
        "    \"<pad\", #preenchimento de valores vazio\n",
        "    \"</s>\", #final de frase\n",
        "    \"<unk>\", #caracter desconhecido\n",
        "    \"<mask>\", #mascara mask\n",
        "])\n",
        ""
      ],
      "metadata": {
        "id": "hQr8SH25QMwg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alguns testes\n",
        "tokenizer.encode(\"Hoje é um novo dia!\").ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJEypI1JRVwn",
        "outputId": "61d21f02-cd5e-46ad-b3e3-f6b4133f269a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[44, 83, 570, 306, 300, 1714, 556, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!rm -r ./sample_data/RAW_MODEL\n",
        "!mkdir ./sample_data/RAW_MODEL\n",
        "tokenizer.save_model(PATH+'RAW_MODEL')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk7XQOEATljH",
        "outputId": "5a0e5545-8671-45c9-cea1-c81f8de91fee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './sample_data/RAW_MODEL': No such file or directory\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./sample_data/RAW_MODEL/vocab.json', './sample_data/RAW_MODEL/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing"
      ],
      "metadata": {
        "id": "yAyiDZY9VJQl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    PATH+'RAW_MODEL' + \"/vocab.json\",\n",
        "    PATH+'RAW_MODEL' + \"/merges.txt\",\n",
        ")\n",
        "\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "2q2SAfDQVL0W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usando nosso tokenizer\n",
        "#https://huggingface.co/docs/transformers/tokenizer_summary\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(PATH+'RAW_MODEL', max_len=512)\n",
        ""
      ],
      "metadata": {
        "id": "gOHt1al2XX3J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurando um transformer tipo Roberta**"
      ],
      "metadata": {
        "id": "QpXt_90bXvGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=512,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")\n",
        "\n",
        "from transformers import RobertaForMaskedLM\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "metadata": {
        "id": "R6f4cTyMXtXu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantos parâmetros tem nossa rede neural\n",
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biCgudhhZezC",
        "outputId": "154532e7-2e25-4877-e793-b81acfd5c182"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83502880"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#forma simples de se carregar um arquivo bruto como Dataset\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=PATH+dados_treino,\n",
        "    block_size=128,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvyOkLWAgoeH",
        "outputId": "55baa7a2-7387-4aac-aa59-3ccea38e9608"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.examples[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stYOsl8og0e6",
        "outputId": "83c4c35f-fba5-424e-c936-0316e9112cff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input_ids': tensor([   0,   69,  276, 1154,  341,  306,  277,  273,   73,  271,  446, 1058,\n",
              "            18,  352,   35, 1155,  262, 1058,  300,  527, 2240,   35,    2])},\n",
              " {'input_ids': tensor([   0,   83,  358, 1142, 3664, 1816,  272,  687, 2688,  781, 4651,  315,\n",
              "          2377,  271, 2768, 1635,  285,  811, 2375,  527,    2])}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Adicionando o data collators para language model**"
      ],
      "metadata": {
        "id": "7lQ4As2xhyoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "QTwRUgYRhwQT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurando um trainer**"
      ],
      "metadata": {
        "id": "mnepqbzIiW7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=PATH+'RAW_MODEL',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1200,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "5aaGYxFKim1t",
        "outputId": "33f7132a-ad56-4fda-b12d-fdbdf63b3405"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-ad4d400b7382>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'RAW_MODEL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mlu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"npu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \"\"\"\n\u001b[1;32m   2093\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2000\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2001\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2002\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "SdRcKgv5j_nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ntjhwoEakK_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}